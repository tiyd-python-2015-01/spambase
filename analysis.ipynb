{
 "metadata": {
  "name": "",
  "signature": "sha256:ade2daf4e7611fff149bab20f560a29acd0005c30729136b33c054c2a5bc55b7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifing spam"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bernoulli naive Bayes vs. Support Vector Machines"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "by Nick Foster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn\n",
      "import pandas as pd\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.linear_model.logistic import LogisticRegression\n",
      "from sklearn import preprocessing\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.svm import SVC\n",
      "from numpy import genfromtxt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = 'input_data/spambase.data'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_data = genfromtxt(path, delimiter=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_data[:1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 127,
       "text": [
        "array([[   0.   ,    0.64 ,    0.64 ,    0.   ,    0.32 ,    0.   ,\n",
        "           0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.64 ,\n",
        "           0.   ,    0.   ,    0.   ,    0.32 ,    0.   ,    1.29 ,\n",
        "           1.93 ,    0.   ,    0.96 ,    0.   ,    0.   ,    0.   ,\n",
        "           0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,\n",
        "           0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,\n",
        "           0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,\n",
        "           0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,\n",
        "           0.   ,    0.   ,    0.   ,    0.778,    0.   ,    0.   ,\n",
        "           3.756,   61.   ,  278.   ,    1.   ]])"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_data[:,-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 128,
       "text": [
        "array([ 1.,  1.,  1., ...,  0.,  0.,  0.])"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(raw_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 193,
       "text": [
        "4601"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Standardizing the dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = preprocessing.scale(raw_data[:, 0:-1])\n",
      "target = raw_data[:, -1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 130,
       "text": [
        "array([[ -3.42433707e-01,   3.30884903e-01,   7.12858774e-01, ...,\n",
        "         -4.52472762e-02,   4.52979198e-02,  -8.72413388e-03],\n",
        "       [  3.45359395e-01,   5.19091945e-02,   4.35129540e-01, ...,\n",
        "         -2.44326749e-03,   2.50562832e-01,   1.22832407e+00],\n",
        "       [ -1.45921392e-01,  -1.65071912e-01,   8.51723390e-01, ...,\n",
        "          1.45920848e-01,   2.22110599e+00,   3.25873251e+00],\n",
        "       ..., \n",
        "       [  6.40127868e-01,  -1.65071912e-01,   3.83734930e-02, ...,\n",
        "         -1.19382054e-01,  -2.36941335e-01,  -2.72627750e-01],\n",
        "       [  2.80176333e+00,  -1.65071912e-01,  -5.56760578e-01, ...,\n",
        "         -1.27482666e-01,  -2.42072958e-01,  -3.38603654e-01],\n",
        "       [ -3.42433707e-01,  -1.65071912e-01,   7.32696576e-01, ...,\n",
        "         -1.24236117e-01,  -2.42072958e-01,  -4.01280763e-01]])"
       ]
      }
     ],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 131,
       "text": [
        "array([ 1.,  1.,  1., ...,  0.,  0.,  0.])"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Splitting the data set into training and test sets. `train_test_split()` will assign 40% of the samples to the test set and rest to the the train set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bernoulli naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence features are used rather than term frequencies. If $x_i$ is a boolean expressing the occurrence or absence of the $i$'th term from the vocabulary, then the likelihood of a document given a class $C_k$ is given by:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$p(\\mathbf{x} \\vert C_k) = \\prod_{i=1}^n p_{ki}^{x_i} (1 - p_{ki})^{(1-x_i)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $p_{ki}$ is the probability of class $C_k$ generating the term $w_i$. This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms. Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial $NB$ classifier with frequency counts truncated to one."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bayes = 'bernoulli'\n",
      "\n",
      "def bernoulli(X_train, y_train):\n",
      "    nb = BernoulliNB()\n",
      "    nb.fit(X_train, y_train)\n",
      "    return nb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_algorithm2 = globals()[bayes]\n",
      "clf = training_algorithm2(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 179,
       "text": [
        "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Making a prediction\n",
      "y_pred = clf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Precision: \\t\", metrics.precision_score(y_test, y_pred))\n",
      "print(\"Recall: \\t\", metrics.recall_score(y_test, y_pred))\n",
      "print(\"F1 score: \\t\", metrics.f1_score(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Precision: \t 0.906060606061\n",
        "Recall: \t 0.829403606103\n",
        "F1 score: \t 0.8660391021\n"
       ]
      }
     ],
     "prompt_number": 181
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Support vector machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. The (Gaussian) radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine classification. Radial basis functions are typically used to build up function approximations of the form where the approximating function $y(x)$ is represented as a sum of $N$ radial basis functions, each associated with a different center $xi$, and weighted by an appropriate coefficient wi. The weights $wi$ can be estimated using the matrix methods of linear least squares, because the approximating function is linear in the weights."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$y(\\mathbf{x}) = \\sum_{i=1}^N w_i \\, \\phi(\\|\\mathbf{x} - \\mathbf{x}_i\\|),$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "algorithm = 'svc_rbf'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def svc_rbf(X_train, y_train):\n",
      "    svc = SVC()\n",
      "    svc.fit(X_train, y_train)\n",
      "    return svc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_algorithm = globals()[algorithm]\n",
      "clf = training_algorithm(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 190,
       "text": [
        "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Making a prediction\n",
      "y_pred = clf.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Precision: \\t\", metrics.precision_score(y_test, y_pred))\n",
      "print(\"Recall: \\t\", metrics.recall_score(y_test, y_pred))\n",
      "print(\"F1 score: \\t\", metrics.f1_score(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Precision: \t 0.939705882353\n",
        "Recall: \t 0.886269070735\n",
        "F1 score: \t 0.912205567452\n"
       ]
      }
     ],
     "prompt_number": 192
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SVM-RBF does a more precise job than Bernoulli naive Bayes with a higher precision, recall, and F1 score. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}